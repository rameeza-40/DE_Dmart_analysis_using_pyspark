# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oo-XUnOd3W5vQcXp0ZY6iLRuck8Dhvuh
"""

!pip install pyspark

import pyspark

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType
from pyspark.sql.functions import col, sum, avg, countDistinct, max

# Task 1: Establish PySpark Connection

spark = SparkSession.builder.appName("Dmart Analysis").getOrCreate()

#Task2: Load Data into Pyspark Connection

products_df = spark.read.csv("/content/Product.csv", header=True, inferSchema=True)
sales_df = spark.read.csv("/content/Sales.csv", header=True, inferSchema=True)
customers_df = spark.read.csv("/content/Customer.csv", header=True, inferSchema=True)

# Show DataFrames to check the data
products_df.show()
sales_df.show()
customers_df.show()

"""# ***Task 3:  Data Transformation and cleaning***

"""

#Task 3:Data Transformation and cleaning

# Rename columns to have a consistent naming convention
products_df=products_df.withColumnRenamed("Product ID", "Product_id").withColumnRenamed("Customer ID", "Customer_id").withColumnRenamed("Product Name", "Product_name")
sales_df=sales_df.withColumnRenamed("Product_ID", "Product_id").withColumnRenamed("Customer ID", "Customer_id").withColumnRenamed("Order Date", "Order_date").withColumnRenamed("Ship Date", "Ship_date").withColumnRenamed("Ship Mode","Ship_mode")
customers_df=customers_df.withColumnRenamed("Customer ID", "Customer_id").withColumnRenamed("Customer Name", "Customer_name")


products_df.show(truncate=False)
sales_df.show(truncate=False)
customers_df.show(truncate=False)

#Handling missing values:
products_df = products_df.dropna()
sales_df = sales_df.dropna()
customers_df = customers_df.dropna()

products_df.show()
sales_df.show()
customers_df.show()

#Ensure data types are correctly set for each column

schema_of_csv =StructType([
    StructField("Product_id", IntegerType(), True),
    StructField("Product_name", StringType(), True),
    StructField("Category", FloatType(), True),
    StructField("Sub-Category", StringType(), True),
    StructField("Order ID", IntegerType(), True),
    StructField("Ship_date", DateType(), True),
    StructField("Ship_Mode", StringType(), True),
    StructField("Customer_id", IntegerType(), True),
    StructField("Sales", FloatType(), True),
    StructField("Quantity", IntegerType(), True),
    StructField("Discount", FloatType(), True),
    StructField("Profit", FloatType(), True),
    StructField("Customer_name", StringType(), True),
    StructField("Segment", StringType(), True),
    StructField("Country", StringType(), True),
    StructField("City", StringType(), True),
    StructField("State", StringType(), True),
    StructField("Postal_Code", IntegerType(), True),
    StructField("Region", StringType(), True),
])

products_df.show(truncate=False)
sales_df.show(truncate=False)
customers_df.show(truncate=False)

#Join the DataFrames on relevant keys (Product ID and Customer ID).

#inner join on two dataframes-Product ID:
Product_sales_df = products_df.join(sales_df, \
               products_df["Product_id"] == sales_df["Product ID"], \
               "inner")
Product_sales_df.show()

# inner join on two dataframes-CustomerID
Customer_sales_df=customers_df.join(sales_df,
               customers_df.Customer_id == sales_df.Customer_id,
               "inner")
Customer_sales_df.show()

#Join Product_sales_df and Customer_sales_df to include all information in one DataFrame
all_data_df = Product_sales_df.join(Customer_sales_df,
                                  on=['Order ID', 'Customer_id', 'Product_id', 'Sales', 'Quantity', 'Discount', 'Profit', 'Order_date', 'Ship_date', 'Ship_mode'],
                                  how='inner')
all_data_df.show()

""" *Task 4 & 5: Data Analysis and Querying*"""

#Task 4 & 5: Data Analysis and Querying

#1. Total Sales for each product category
total_sales_category = Product_sales_df.groupBy("Category").sum("Sales")
total_sales_category.show()

#2. Customer with the highest number of purchases:
customer_purchases = Customer_sales_df.groupBy(customers_df["Customer_id"]).count().orderBy(col("count").desc())
customer_purchases.show(1)

#3. Average discount given
avg_discount = Product_sales_df.select(avg("Discount"))
avg_discount.show()

#4. Unique products sold into each region:
unique_products_region = all_data_df.groupBy("Region").agg(countDistinct(products_df["Product_id"]).alias("Unique_Products"))
unique_products_region.show()

# 5. Total profit generated in each state
total_profit_state = all_data_df.groupBy("State").sum("Profit")
total_profit_state.show()

# 6. Product sub-category with the highest sales
highest_sales_subcategory = all_data_df.groupBy("Sub-Category").sum("Sales").orderBy(col("sum(Sales)").desc())
highest_sales_subcategory.show(1)

# 7. Average age of customers in each segment:
avg_age_segment = all_data_df.groupBy("Segment").avg("Age")
avg_age_segment.show()

# 8. Orders shipped in each shipping mode
orders_shipping_mode = all_data_df.groupBy("Ship_Mode").count()
orders_shipping_mode.show()

# 9. Total quantity of products sold in each city
total_qty_city = all_data_df.groupBy("City").sum("Quantity")
total_qty_city.show()

# 10. Customer segment with the highest profit margin
profit_segment = all_data_df.groupBy("Segment").sum("Profit").orderBy(col("sum(Profit)").desc())
profit_segment.show(1)

# Stop Spark session
spark.stop()

"""==================***Project- 5 Dmart analysis using pyspark-ended***======================

"""

